{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61ef28b-f07f-46c0-8ec0-7f1cb939a841",
   "metadata": {},
   "source": [
    "## (주)다유푸드 제품 2개 리뷰 크롤링 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca551a9d-0bc0-4ea3-9747-866d951e0782",
   "metadata": {},
   "source": [
    " ### 당플랜 소고기 볶음밥, 당플랜 돼지고기 볶음밥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad4c26a-d866-49c6-b784-e1b184cf51fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffea9744-e370-4d78-859f-2d46d23834e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 리뷰가 C:\\Users\\USER\\Desktop\\5_6\\core_pj\\data\\all_reviews.txt에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로\n",
    "file_path = r'C:\\Users\\USER\\Desktop\\5_6\\core_pj\\data\\crawlingOrNogada\\hyundaiGreenFoodAllReview.xlsx'\n",
    "\n",
    "# 엑셀 파일 로드\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 모든 리뷰 데이터를 하나의 텍스트 파일로 저장\n",
    "output_file_path = r'C:\\Users\\USER\\Desktop\\5_6\\core_pj\\data\\all_reviews.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    for index, row in df.iterrows():\n",
    "        review_text = str(row['리뷰']) if not pd.isna(row['리뷰']) else \"\"\n",
    "        file.write(review_text + '\\n')\n",
    "\n",
    "print(f\"모든 리뷰가 {output_file_path}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf3f1124-f109-419e-a933-9dec3fbc9f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Exception occurred: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"#review-list > ul > li.page-item.next > a\"}\n",
      "  (Session info: chrome=124.0.6367.210); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0070C113+48259]\n",
      "\t(No symbol) [0x0069CA41]\n",
      "\t(No symbol) [0x00590A17]\n",
      "\t(No symbol) [0x005D0BED]\n",
      "\t(No symbol) [0x005D0C9B]\n",
      "\t(No symbol) [0x0060BC12]\n",
      "\t(No symbol) [0x005F0DE4]\n",
      "\t(No symbol) [0x00609B9C]\n",
      "\t(No symbol) [0x005F0B36]\n",
      "\t(No symbol) [0x005C570D]\n",
      "\t(No symbol) [0x005C62CD]\n",
      "\tGetHandleVerifier [0x009C65A3+2908435]\n",
      "\tGetHandleVerifier [0x00A03BBB+3159851]\n",
      "\tGetHandleVerifier [0x007A50CB+674875]\n",
      "\tGetHandleVerifier [0x007AB28C+699900]\n",
      "\t(No symbol) [0x006A6244]\n",
      "\t(No symbol) [0x006A2298]\n",
      "\t(No symbol) [0x006A242C]\n",
      "\t(No symbol) [0x00694BB0]\n",
      "\tBaseThreadInitThunk [0x763C7BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x77ACBE3B+107]\n",
      "\tRtlClearBits [0x77ACBDBF+191]\n",
      "\n",
      "           Product Name                                             Review\n",
      "0    당플랜 닭가슴살 볶음밥 (6EA)                                      늘 잘먹고 있습니다~^^\n",
      "1    당플랜 닭가슴살 볶음밥 (6EA)                                      맛있어요. 양도 적당해요\n",
      "2    당플랜 닭가슴살 볶음밥 (6EA)  5월 5일에 주문했습니다.\\n도착은 9일 13시 42분에 도착했습니다.\\n6일까지 ...\n",
      "3    당플랜 닭가슴살 볶음밥 (6EA)  너무 맛있게 잘먹고 있습니다.\\n다른 볶음밥에 비해 양도 많이 들어있고 당뇨** 용...\n",
      "4    당플랜 닭가슴살 볶음밥 (6EA)                                   맛있어요. 잘 먹고 있습니다.\n",
      "..                  ...                                                ...\n",
      "201  당플랜 닭가슴살 볶음밥 (6EA)  다 먹으면 항상 또 구매해서 먹는 볶음밥~ 개인적으로 최애 볶음밥 ㅎ 당뇨진단 받고...\n",
      "202  당플랜 닭가슴살 볶음밥 (6EA)                                         점심대용으로 좋아요\n",
      "203  당플랜 닭가슴살 볶음밥 (6EA)            간단하게 챙겨먹기 좋아서 재주문했습니다~\\n이벤트 자주자주 해주세요^^\n",
      "204  당플랜 닭가슴살 볶음밥 (6EA)                              아직 먹기전인데 다 녹아서 왔어요 ㅜㅜ\n",
      "205  당플랜 닭가슴살 볶음밥 (6EA)                                     자극적이지 않은 맛입니다.\n",
      "\n",
      "[206 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver as wb\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 드라이버 설정\n",
    "driver = wb.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# 사이트 열기\n",
    "url = \"https://www.wellife.co.kr/products/view/G2001000864#enp_mbris\"\n",
    "# 다유푸드 제품 해당 url만 바꿔서 넣어주면 리뷰 긁어올 수 있음\n",
    "driver.get(url)\n",
    "time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "# 광고 페이지 닫기\n",
    "ad_close_selector = '#is2popup1 > div.is2_popup-content > div > div.is2_popup-buttons > button.is2_popup-button.is2_popup-button-today'\n",
    "try:\n",
    "    ad_close_button = driver.find_element(By.CSS_SELECTOR, ad_close_selector)\n",
    "    ad_close_button.click()\n",
    "    time.sleep(1)  # 광고 닫기 대기\n",
    "except Exception as e:\n",
    "    print(\"광고 페이지가 발견되지 않음:\", e)\n",
    "\n",
    "# 후기 탭 클릭\n",
    "review_tab_selector = '#saleson > section > div.item_tab.widowScrolled > div > ul > li:nth-child(3) > a > span.txt'\n",
    "review_tab = driver.find_element(By.CSS_SELECTOR, review_tab_selector)\n",
    "\n",
    "# 스크롤 이동 후 클릭\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", review_tab)\n",
    "time.sleep(1)\n",
    "driver.execute_script(\"arguments[0].click();\", review_tab)\n",
    "time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "# 상품명 가져오기\n",
    "product_name_selector = '#saleson > section > div.view_top.container > div > div.col-12.col-lg-6.view_info > div.tit_area > p.title'\n",
    "product_name = driver.find_element(By.CSS_SELECTOR, product_name_selector).text\n",
    "\n",
    "reviews = []\n",
    "page = 1\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(f\"Scraping page {page}...\")\n",
    "\n",
    "        # 리뷰 텍스트 가져오기\n",
    "        review_texts = driver.find_elements(By.CSS_SELECTOR, '#review-list > li > div > div > div.review_origin > div.txt_area > div')\n",
    "        for review in review_texts:\n",
    "            reviews.append(review.text)\n",
    "\n",
    "        # 다음 페이지 버튼 클릭\n",
    "        next_button = driver.find_element(By.CSS_SELECTOR, '#review-list > ul > li.page-item.next > a')\n",
    "        if next_button.is_displayed():\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            time.sleep(2)  # 페이지 로딩 대기\n",
    "            page += 1\n",
    "        else:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        break\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(reviews, columns=[\"Review\"])\n",
    "df.insert(0, \"Product Name\", product_name)\n",
    "\n",
    "# 크롤링 결과 출력\n",
    "print(df)\n",
    "\n",
    "# 크롬 드라이버 종료\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea100ed3-7d23-40bc-815b-2323491a36ab",
   "metadata": {},
   "source": [
    "이렇게 수집해서 df를 엑셀로 저장해놓고 ' (6ea)' 부분 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcddcb53-2045-4e15-8815-e4f04991b727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ec725a1-a956-4f20-9725-71f6fc65ed72",
   "metadata": {},
   "source": [
    "## 나중에 리뷰 분류를 직접 해보고 싶다면 이런식으로 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9618d-76fa-4eaa-a1be-703ad13400ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 파일 경로\n",
    "file_path = r'C:\\Users\\USER\\Desktop\\5_6\\core_pj\\data\\crawlingOrNogada\\hyundaiGreenFoodAllReview.xlsx'\n",
    "\n",
    "# 엑셀 파일 로드\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 예시로 감성 레이블을 추가 (데이터에 실제 레이블이 있어야 함)\n",
    "# 실제 데이터에는 수동으로 레이블링하거나 다른 방법으로 레이블링해야 합니다.\n",
    "# 여기서는 임의로 0 (부정), 1 (긍정) 레이블을 추가합니다.\n",
    "# df['sentiment'] = [1, 0, 1, 0, 1]  # 예시 레이블, 실제 데이터에 맞게 수정 필요\n",
    "\n",
    "# 데이터와 레이블 분리\n",
    "X = df['리뷰']\n",
    "# y = df['sentiment']\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 텍스트 벡터화 (TF-IDF)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# 로지스틱 회귀 모델 학습\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "# 결과 출력\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 감성 점수를 -2에서 2까지의 범주로 변환\n",
    "def sentiment_to_category(score):\n",
    "    if score >= 1.5:\n",
    "        return 2\n",
    "    elif score >= 0.5:\n",
    "        return 1\n",
    "    elif score >= -0.5:\n",
    "        return 0\n",
    "    elif score >= -1.5:\n",
    "        return -1\n",
    "    else:\n",
    "        return -2\n",
    "\n",
    "# 예측 점수를 범주로 변환하여 데이터프레임에 추가\n",
    "df['predicted_sentiment'] = model.predict(vectorizer.transform(df['리뷰']))\n",
    "df['sentiment_category'] = df['predicted_sentiment'].apply(sentiment_to_category)\n",
    "\n",
    "# 결과 엑셀 파일로 저장\n",
    "output_file_path = r'C:\\Users\\USER\\Desktop\\5_6\\core_pj\\data\\crawlingOrNogada\\hyundaiGreenFoodSentimentAnalysis.xlsx'\n",
    "df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"결과가 {output_file_path}에 저장되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
